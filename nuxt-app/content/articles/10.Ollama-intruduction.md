---
cover: https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA
date: 2024-07-28T00:00:00.000Z
description: How To run AI Models on your own PC | Beginner Guide
layout: article_backup
order: 2
---





<style>
.ollama-navbar {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
}
.ollama-navbar-btn {
    background: #007bff;
    color: white;
    border: none;
    border-radius: 6px 6px 0 0;
    padding: 12px 32px;
    font-size: 1em;
    font-weight: bold;
    cursor: pointer;
    transition: background 0.2s;
    outline: none;
}
.ollama-navbar-btn.active,
.ollama-navbar-btn:hover {
    background: #0056b3;
}
.ollama-navbar-content {
    background: #000000ff;
    border-radius: 0 0 8px 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.08);
    padding: 24px;
    margin-top: -2px;
    border-top: 1px solid #ddd;
}
</style>


<div class="ollama-navbar-tabs">
<input type="radio" name="ollama-tab" id="ollama-tab-0" class="ollama-tab-radio" checked>
<input type="radio" name="ollama-tab" id="ollama-tab-1" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-2" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-3" class="ollama-tab-radio">
<div class="ollama-navbar">
    <label class="ollama-navbar-btn" for="ollama-tab-0">üìã Preparation</label>
    <label class="ollama-navbar-btn" for="ollama-tab-1">‚öôÔ∏è Setup</label>
    <label class="ollama-navbar-btn" for="ollama-tab-2">üéÆ Model Playground</label>
    <label class="ollama-navbar-btn" for="ollama-tab-3">üîß Troubleshooting</label>
</div>
<div class="ollama-navbar-content ollama-content-0">
    <h4>Before You Start</h4>
    <table>
        <tr>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/V2oVVCsPaNxkGlhMMz5AxhgzgEvkZRmyQf3x22R5ebQ/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuc3RpY2twbmcu/Y29tL2ltYWdlcy82/MmE3OTA2Y2U0MmQ3/MjlkOTI4YjE3NTcu/cG5n" alt="VS Code Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://code.visualstudio.com/download" target="_blank" rel="noopener">VS Code Download</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://www.python.org/static/community_logos/python-logo.png" alt="Python Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://www.python.org/downloads/" target="_blank" rel="noopener">Python Download</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA" alt="Ollama Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://ollama.com/download" target="_blank" rel="noopener">Ollama Download</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/zLvtdX6w_dNUl6wAzFN-0BCdZQrJu7VkSySkbESjtsc/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy9i/L2I5L052aWRpYV9D/VURBX0xvZ28uanBn" alt="CUDA Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA Download</a>
            </td>
        </tr>
    </table>
    <table>
        <thead>
            <tr>
                <th>Component</th>
                <th>Basic</th>
                <th>Enthusiast</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>RAM</td>
                <td>16 GB</td>
                <td>32 GB+</td>
            </tr>
            <tr>
                <td>GPU VRAM</td>
                <td>8 GB</td>
                <td>16 GB+</td>
            </tr>
            <tr>
                <td>Storage</td>
                <td>512 GB SSD</td>
                <td>1 TB+ SSD</td>
            </tr>
        </tbody>
    </table>
    <p>     </p>
        <div style="margin-top: 24px; margin-bottom: 24px;">
            <h4>Before You Begin: A Warm Welcome for Beginners</h4>
            <p>Hello and welcome! If you are opening this guide, you have probably heard a lot about artificial intelligence and now want to get your hands on it yourself. But perhaps you also feel a little overwhelmed by terms like \"local LLMs,\" \"Jupyter Notebooks,\" and \"Virtual Environments.\" Don't worry, that is completely normal! This tutorial was written specifically for absolute beginners like you.</p>
            <p>Our goal is to show you how to run powerful AI models directly on your own computer. This is not only incredibly fascinating, but it also offers you full control over your data, maximum privacy, and the freedom to use AI models without an internet connection or expensive cloud services.</p>
            <h5>What to Expect: The Path to Local AI</h5>
            <p>Imagine you have a brand new toolkit and are ready to build something big. Ollama is the most important tool in this kit. It takes the complexity out of the equation and makes it easy to install, manage, and run various AI models. We will use this tool to set up an AI \"playground\" in Visual Studio Code (VS Code), which resembles a kind of lab book called a Jupyter Notebook.</p>
            <p>In the following steps, we will together:</p>
            <ul>
                <li>Install the necessary software.</li>
                <li>Set up your development environment so that everything is clean and organized.</li>
                <li>Download your first AI model and get it running with a few lines of code.</li>
            </ul>
            <p>You don't need to be a programming expert to get started. We will guide you through every single step and provide you with all the code snippets you need.</p>
            <h5>A Quick Look at the Hardware</h5>
            <p>You saw a table with hardware recommendations in the previous overview. It is important to understand that the performance of AI models heavily depends on your Random Access Memory (RAM) and, if you have one, on your Graphics Processing Unit (GPU).</p>
            <ul>
                <li><strong>RAM:</strong> This is your computer's short-term memory. The larger the AI model, the more RAM is needed to run it.</li>
                <li><strong>GPU:</strong> An NVIDIA graphics card can massively accelerate computational power. If you have one, the AI experience will be significantly faster. But even without a powerful GPU, you can get started‚Äîit will just take a bit longer.</li>
            </ul>
            <p>See these recommendations as a guide, not as rigid rules. Even with a basic setup, you can experiment with smaller, yet impressive, models and learn the fundamentals.</p>
            <h5>The Most Important Thing: Be Patient and Be Curious!</h5>
            <p>This is a learning process. It's possible you will encounter an error message or that something doesn't work right away. Don't worry! This is why we have created a special Troubleshooting section that provides solutions for the most common problems. The greatest reward awaits those who remain curious and enjoy experimenting.</p>
        </div>
</div>
<div class="ollama-navbar-content ollama-content-1">
    <h4>Installation Process</h4>
    <ol>
        <li>Install VS Code extensions: <strong>Python</strong> and <strong>Jupyter</strong> (Ctrl+Shift+X in VS Code).</li>
        <li>Create a virtual Python environment: Open folder in VS Code, use <code>Python: Create Environment</code> (Ctrl+Shift+P), select <code>Venv</code>.</li>
        <li>Install Jupyter kernel: Open terminal, run <code>pip install ipykernel</code> in your virtual environment.</li>
    </ol>
    <p>Start Ollama after installation and check if the service is running. Make sure Python and CUDA are set up correctly for optimal AI model performance. Configure your environment variables and test the installation.</p>
</div>
<div class="ollama-navbar-content ollama-content-2">
    <h4>Testing and Experimentation</h4>
    <p>Compare model requirements:</p>
    <ul>
        <li><strong>gemma3:1b</strong> ‚Äì 815 MB, 4 GB RAM, Multimodal/Chat</li>
        <li><strong>mistral:7b</strong> ‚Äì 4.1 GB, 8 GB RAM, Fast Chat</li>
        <li><strong>llama2:7b</strong> ‚Äì 3.8 GB, 8 GB RAM, General Chat</li>
        <li><strong>codellama:7b</strong> ‚Äì 3.8 GB, 8 GB RAM, Code Generation</li>
        <li><strong>deepseek-coder:33b</strong> ‚Äì 18 GB, 22 GB RAM, Complex Coding</li>
        <li><strong>llama3.1:70b</strong> ‚Äì 40 GB, 48 GB RAM, Advanced Reasoning</li>
    </ul>
    <ol>
        <li>Start Ollama server in a separate terminal: <code>ollama serve</code></li>
        <li>Download a model: <code>ollama pull gemma3:1b</code></li>
        <li>Run in Jupyter Notebook (VS Code):</li>

        # Ensure Ollama server is running!
        model_name = 'gemma3:1b'

        try:
            ollama.list()
            print("Ollama Server reachable.")

            response = ollama.chat(
                model=model_name,
                messages=[
                    {
                        'role': 'user',
                        'content': 'Why is the sky blue? Explain simply.',
                    },
                ]
            )
            print("\nFull response:")
            print(response)
            # Robust: Zeige die Antwort, falls das Feld existiert
            if 'message' in response and isinstance(response['message'], dict) and 'content' in response['message']:
                print("\nModel response:")
                print(response['message']['content'])
            else:
                print("\nNo valid model response found in 'message' field.")
        except Exception as e:
            print(f"\nError: {e}")
            print("Make sure 'ollama serve' is running in a separate terminal.")
</ol>   
</div>
<div class="ollama-navbar-content ollama-content-3">
    <h4>Common Issues and Solutions</h4>
    <ul>
        <li><strong>ModuleNotFoundError: No module named 'ollama'</strong><br>
            Install <code>ollama</code> in your active virtual environment: <code>pip install ollama</code>.
        </li>
        <li><strong>Slow performance or high CPU usage</strong><br>
            Try smaller models (e.g., <code>gemma3:1b</code>). For NVIDIA GPUs, ensure CUDA Toolkit is installed.
        </li>
        <li><strong>Connection to Ollama server failed</strong><br>
            Make sure <code>ollama serve</code> is running in a separate terminal. Check firewall settings for port 11434.
        </li>
        <li><strong>CUDA conflicts or GPU not detected</strong><br>
            Update NVIDIA drivers, check CUDA version with <code>nvidia-smi</code>, and restart your system if needed.
        </li>
    </ul>
    <p>Refer to documentation or forums for further help.</p>
</div>
</div>
<style>
.ollama-navbar-tabs {
  width: 100%;
}
.ollama-tab-radio {
  display: none;
}
.ollama-navbar-btn {
  /* ...bestehendes Styling... */
}
.ollama-navbar-content {
  display: none;
}
#ollama-tab-0:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-0"],
#ollama-tab-1:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-1"],
#ollama-tab-2:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-2"],
#ollama-tab-3:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-3"] {
  background: #0056b3;
}
#ollama-tab-0:checked ~ .ollama-content-0,
#ollama-tab-1:checked ~ .ollama-content-1,
#ollama-tab-2:checked ~ .ollama-content-2,
#ollama-tab-3:checked ~ .ollama-content-3 {
  display: block;
}
</style>
