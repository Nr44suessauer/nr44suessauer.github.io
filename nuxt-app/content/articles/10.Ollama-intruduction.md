---
cover: https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA
date: 2024-07-28T00:00:00.000Z
description: How To run AI Models on your own PC | Beginner Guide
layout: article_backup
order: 2
---


<table>
    <tr>
        <td align="center" style="width: 180px;">
            <img src="https://www.python.org/static/community_logos/python-logo.png" alt="Python Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
            <br>
            <a href="https://www.python.org/downloads/">Python Download</a>
        </td>
        <td align="center" style="width: 180px;">
            <img src="https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA" alt="Ollama Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
            <br>
            <a href="https://ollama.com/download">Ollama Download</a>
        </td>
        <td align="center" style="width: 180px;">
            <img src="https://imgs.search.brave.com/zLvtdX6w_dNUl6wAzFN-0BCdZQrJu7VkSySkbESjtsc/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy9i/L2I5L052aWRpYV9D/VURBX0xvZ28uanBn" alt="CUDA Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
            <br>
            <a href="https://developer.nvidia.com/cuda-downloads?target_os=Windows">CUDA Download</a>
        </td>
    </tr>
</table>

---

<style>
.ollama-navbar {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
}
.ollama-navbar-btn {
    background: #007bff;
    color: white;
    border: none;
    border-radius: 6px 6px 0 0;
    padding: 12px 32px;
    font-size: 1em;
    font-weight: bold;
    cursor: pointer;
    transition: background 0.2s;
    outline: none;
}
.ollama-navbar-btn.active,
.ollama-navbar-btn:hover {
    background: #0056b3;
}
.ollama-navbar-content {
    background: #000000ff;
    border-radius: 0 0 8px 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.08);
    padding: 24px;
    margin-top: -2px;
    border-top: 1px solid #ddd;
}
</style>


<div class="ollama-navbar-tabs">
<input type="radio" name="ollama-tab" id="ollama-tab-0" class="ollama-tab-radio" checked>
<input type="radio" name="ollama-tab" id="ollama-tab-1" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-2" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-3" class="ollama-tab-radio">
<div class="ollama-navbar">
    <label class="ollama-navbar-btn" for="ollama-tab-0">üìã Preparation</label>
    <label class="ollama-navbar-btn" for="ollama-tab-1">‚öôÔ∏è Setup</label>
    <label class="ollama-navbar-btn" for="ollama-tab-2">üéÆ Model Playground</label>
    <label class="ollama-navbar-btn" for="ollama-tab-3">üîß Troubleshooting</label>
</div>
<div class="ollama-navbar-content ollama-content-0">
    <h4>Before You Start</h4>
    <ul>
        <li><strong>Python:</strong> Latest stable version recommended. <a href="https://www.python.org/downloads/">Download</a></li>
        <li><strong>Ollama:</strong> Enables local LLM execution. <a href="https://ollama.com/download">Download</a></li>
        <li><strong>Visual Studio Code:</strong> Editor for code and Jupyter Notebooks. <a href="https://code.visualstudio.com/download">Download</a></li>
        <li><strong>NVIDIA CUDA Toolkit (Optional):</strong> For NVIDIA GPU users, boosts performance. <a href="https://developer.nvidia.com/cuda-downloads">Download</a></li>
    </ul>
    <table>
        <thead>
            <tr>
                <th>Component</th>
                <th>Basic</th>
                <th>Enthusiast</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>RAM</td>
                <td>16 GB</td>
                <td>32 GB+</td>
            </tr>
            <tr>
                <td>GPU VRAM</td>
                <td>8 GB</td>
                <td>16 GB+</td>
            </tr>
            <tr>
                <td>Storage</td>
                <td>512 GB SSD</td>
                <td>1 TB+ SSD</td>
            </tr>
        </tbody>
    </table>
    <p>Check your system specs and install the required software before proceeding.</p>
</div>
<div class="ollama-navbar-content ollama-content-1">
    <h4>Installation Process</h4>
    <ol>
        <li>Install VS Code extensions: <strong>Python</strong> and <strong>Jupyter</strong> (Ctrl+Shift+X in VS Code).</li>
        <li>Create a virtual Python environment: Open folder in VS Code, use <code>Python: Create Environment</code> (Ctrl+Shift+P), select <code>Venv</code>.</li>
        <li>Install Jupyter kernel: Open terminal, run <code>pip install ipykernel</code> in your virtual environment.</li>
    </ol>
    <p>Start Ollama after installation and check if the service is running. Make sure Python and CUDA are set up correctly for optimal AI model performance. Configure your environment variables and test the installation.</p>
</div>
<div class="ollama-navbar-content ollama-content-2">
    <h4>Testing and Experimentation</h4>
    <p>Compare model requirements:</p>
    <ul>
        <li><strong>gemma3:1b</strong> ‚Äì 815 MB, 4 GB RAM, Multimodal/Chat</li>
        <li><strong>mistral:7b</strong> ‚Äì 4.1 GB, 8 GB RAM, Fast Chat</li>
        <li><strong>llama2:7b</strong> ‚Äì 3.8 GB, 8 GB RAM, General Chat</li>
        <li><strong>codellama:7b</strong> ‚Äì 3.8 GB, 8 GB RAM, Code Generation</li>
        <li><strong>deepseek-coder:33b</strong> ‚Äì 18 GB, 22 GB RAM, Complex Coding</li>
        <li><strong>llama3.1:70b</strong> ‚Äì 40 GB, 48 GB RAM, Advanced Reasoning</li>
    </ul>
    <ol>
        <li>Start Ollama server in a separate terminal: <code>ollama serve</code></li>
        <li>Download a model: <code>ollama pull gemma3:1b</code></li>
        <li>Run in Jupyter Notebook (VS Code):</li>

        # Ensure Ollama server is running!
        model_name = 'gemma3:1b'

        try:
            ollama.list()
            print("Ollama Server reachable.")

            response = ollama.chat(
                model=model_name,
                messages=[
                    {
                        'role': 'user',
                        'content': 'Why is the sky blue? Explain simply.',
                    },
                ]
            )
            print("\nFull response:")
            print(response)
            # Robust: Zeige die Antwort, falls das Feld existiert
            if 'message' in response and isinstance(response['message'], dict) and 'content' in response['message']:
                print("\nModel response:")
                print(response['message']['content'])
            else:
                print("\nNo valid model response found in 'message' field.")
        except Exception as e:
            print(f"\nError: {e}")
            print("Make sure 'ollama serve' is running in a separate terminal.")
</ol>   
</div>
<div class="ollama-navbar-content ollama-content-3">
    <h4>Common Issues and Solutions</h4>
    <ul>
        <li><strong>ModuleNotFoundError: No module named 'ollama'</strong><br>
            Install <code>ollama</code> in your active virtual environment: <code>pip install ollama</code>.
        </li>
        <li><strong>Slow performance or high CPU usage</strong><br>
            Try smaller models (e.g., <code>gemma3:1b</code>). For NVIDIA GPUs, ensure CUDA Toolkit is installed.
        </li>
        <li><strong>Connection to Ollama server failed</strong><br>
            Make sure <code>ollama serve</code> is running in a separate terminal. Check firewall settings for port 11434.
        </li>
        <li><strong>CUDA conflicts or GPU not detected</strong><br>
            Update NVIDIA drivers, check CUDA version with <code>nvidia-smi</code>, and restart your system if needed.
        </li>
    </ul>
    <p>Refer to documentation or forums for further help.</p>
</div>
</div>
<style>
.ollama-navbar-tabs {
  width: 100%;
}
.ollama-tab-radio {
  display: none;
}
.ollama-navbar-btn {
  /* ...bestehendes Styling... */
}
.ollama-navbar-content {
  display: none;
}
#ollama-tab-0:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-0"],
#ollama-tab-1:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-1"],
#ollama-tab-2:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-2"],
#ollama-tab-3:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-3"] {
  background: #0056b3;
}
#ollama-tab-0:checked ~ .ollama-content-0,
#ollama-tab-1:checked ~ .ollama-content-1,
#ollama-tab-2:checked ~ .ollama-content-2,
#ollama-tab-3:checked ~ .ollama-content-3 {
  display: block;
}
</style>
