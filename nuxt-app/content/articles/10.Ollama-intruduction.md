---
cover: https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA
date: 2025-07-01T00:00:00.000Z
description: How To run AI Models on your own PC + Software A1 Terminal for interact with these | Beginner/Setup Guide
layout: article_backup
---


<style>
/* Maximale Breite f√ºr den gesamten Artikel - √ºberschreibt alle Container */
.prose, 
.prose-lg,
.container,
article,
.article-container,
.content-container,
main {
    max-width: 100% !important;
    width: 100% !important;
}

/* Entfernt seitliche Margins und Padding */
.prose, article {
    margin-left: 0 !important;
    margin-right: 0 !important;
    padding-left: 20px !important;
    padding-right: 20px !important;
}

.ollama-navbar-tabs {
    max-width: 100%;
    width: 100%;
}

.ollama-navbar {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
    justify-content: center;
}
.ollama-navbar-btn {
    background: #007bff;
    color: white;
    border: none;
    border-radius: 6px 6px 0 0;
    padding: 12px 32px;
    font-size: 1em;
    font-weight: bold;
    cursor: pointer;
    transition: background 0.2s;
    outline: none;
}
.ollama-navbar-btn.active,
.ollama-navbar-btn:hover {
    background: #0056b3;
}
.ollama-navbar-content {
    background: #000000ff;
    border-radius: 0 0 8px 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.08);
    padding: 24px;
    margin-top: -2px;
    border-top: 1px solid #ddd;
    width: 60%;
    margin-left: auto;
    margin-right: auto;
}

/* Tabellen auf volle Breite */
table {
    width: 100% !important;
}
</style>
<div class="ollama-navbar-tabs">
<input type="radio" name="ollama-tab" id="ollama-tab-0" class="ollama-tab-radio" checked>
<input type="radio" name="ollama-tab" id="ollama-tab-1" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-2" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-3" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-4" class="ollama-tab-radio">
<div class="ollama-navbar">
    <label class="ollama-navbar-btn" for="ollama-tab-0">üß† How AI Works</label>
    <label class="ollama-navbar-btn" for="ollama-tab-1">üìã Preparation</label>
    <label class="ollama-navbar-btn" for="ollama-tab-2">‚öôÔ∏è Setup for developers</label>
    <label class="ollama-navbar-btn" for="ollama-tab-3">üí¨ A1-Terminal</label>
    <label class="ollama-navbar-btn" for="ollama-tab-4">üìñ A1-Manual</label>
</div>
<div class="ollama-navbar-content ollama-content-0">

#### How AI Works: Understanding Large Language Models

##### ü§î What is a Large Language Model (LLM)?

A Large Language Model is an artificial intelligence system trained on vast amounts of text data to understand and generate human-like text. Think of it as a sophisticated pattern recognition system that has "read" billions of pages of text and learned the statistical relationships between words and concepts.

**Key Concepts:**

- **Neural Networks:** LLMs are built on artificial neural networks inspired by the human brain, consisting of billions of interconnected nodes (parameters)
- **Training:** Models learn by processing massive datasets, adjusting their internal parameters to predict the next word in a sequence
- **Inference:** When you ask a question, the model uses its learned patterns to generate a relevant response word by word


##### üîí Why Run AI Locally?

**Privacy:** Your data never leaves your computer - no cloud servers, no logging  
**Cost:** No API fees or subscription costs after initial setup  
**Customization:** Full control over model behavior and system prompts  
**Offline:** Works without internet connection  
**Learning:** Understand how AI systems actually work


##### üìä Model Sizes and Parameters

The "size" of a model refers to the number of parameters (weights) it contains:

| Model Size | Parameters | RAM Required | Example Models |
|------------|------------|--------------|----------------|
| Tiny | 1-2B | 4-8 GB | tinyllama:1.1b, gemma:2b |
| Small | 3-7B | 8-16 GB | phi3:mini, llama3.2:3b, mistral:7b |
| Medium | 13-34B | 24-48 GB | llama2:13b, codellama:34b |
| Large | 70B+ | 80+ GB | llama3.1:70b, mixtral:8x7b |

**More parameters ‚â† Always better:** Smaller, well-trained models can outperform larger ones for specific tasks.

##### ‚öôÔ∏è How Does Text Generation Work?

1. **Tokenization:** Your input text is split into "tokens" (roughly words or word parts)
2. **Embedding:** Each token is converted into a numerical vector
3. **Processing:** These vectors flow through multiple neural network layers
4. **Prediction:** The model calculates probability scores for the next token
5. **Sampling:** A token is selected based on these probabilities (with some randomness for creativity)
6. **Repeat:** Steps 3-5 continue until the response is complete

**Example:**
```
Input: "The capital of France is"
Model thinks: "Paris" (95%), "Lyon" (2%), "Marseille" (1%)...
Output: "Paris"
```

##### üéØ Temperature and Sampling

**Temperature** controls the randomness of responses:

- **Temperature 0.0:** Deterministic, always picks the most likely token (good for factual answers)
- **Temperature 0.7:** Balanced creativity and coherence (default for most tasks)
- **Temperature 1.0+:** More creative/random (good for storytelling, brainstorming)

**Top-K & Top-P Sampling:** Additional techniques to control output quality by limiting which tokens can be selected.

##### üèãÔ∏è CPU vs GPU Processing

**Why GPUs are faster:**

- **Parallel Processing:** GPUs have thousands of cores that can process many calculations simultaneously
- **Matrix Operations:** AI models require massive matrix multiplications, which GPUs excel at
- **VRAM:** GPU memory is faster than system RAM for neural network operations

**CPU Processing:**
- Uses system RAM
- Processes calculations sequentially (or with limited parallelism)
- Works perfectly fine but slower (seconds vs milliseconds per token)

**Practical Impact:**
- **CPU:** 3-10 tokens/second (small models)
- **GPU (8GB):** 20-50 tokens/second
- **GPU (16GB+):** 50-100+ tokens/second

##### üéì Common Misconceptions

‚ùå **"AI understands like humans do"**  
‚úÖ AI recognizes patterns in text but doesn't "understand" meaning in a human sense

‚ùå **"Bigger models are always better"**  
‚úÖ Smaller specialized models often perform better for specific tasks

‚ùå **"AI needs GPU to work"**  
‚úÖ CPU-only operation is perfectly viable, just slower

‚ùå **"AI is always accurate"**  
‚úÖ Models can "hallucinate" (generate plausible-sounding but incorrect information)

##### üìö Recommended Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762){target="_blank"} - The transformer paper that revolutionized AI
- [Ollama Documentation](https://github.com/ollama/ollama/tree/main/docs){target="_blank"} - Technical details about local model deployment
- [Hugging Face Model Hub](https://huggingface.co/models){target="_blank"} - Explore available AI models

**Ready to get started?** Check the next tabs for installation and setup instructions!

</div>

<div class="ollama-navbar-content ollama-content-1">
        <div style="margin-top: 24px; margin-bottom: 24px;">
            <h4>Before You Begin</h4>
            <p>This guide provides an introduction to running AI models locally on your computer. You'll learn how to work with Large Language Models (LLMs) using tools like Ollama, Jupyter Notebooks, and Python virtual environments.</p>
            <p>Running AI models locally offers several advantages: full control over your data, enhanced privacy, no dependency on internet connectivity, and no recurring cloud service costs. This approach is particularly valuable for academic work, research projects, and learning the fundamentals of AI implementation.</p>
        </div>
    <table>
        <tr>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/V2oVVCsPaNxkGlhMMz5AxhgzgEvkZRmyQf3x22R5ebQ/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuc3RpY2twbmcu/Y29tL2ltYWdlcy82/MmE3OTA2Y2U0MmQ3/MjlkOTI4YjE3NTcu/cG5n" alt="VS Code Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://code.visualstudio.com/download" target="_blank" rel="noopener">VS Code</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://www.python.org/static/community_logos/python-logo.png" alt="Python Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://www.python.org/downloads/" target="_blank" rel="noopener">Python</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA" alt="Ollama Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://ollama.com/download" target="_blank" rel="noopener">Ollama</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/zLvtdX6w_dNUl6wAzFN-0BCdZQrJu7VkSySkbESjtsc/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy9i/L2I5L052aWRpYV9D/VURBX0xvZ28uanBn" alt="CUDA Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA</a>
            </td>
        </tr>
    </table>
    <table class="w-full text-left table-auto border-separate [border-spacing:0_0.75rem]">
        <thead class="text-gray-600 uppercase text-sm font-semibold">
            <tr>
                <th class="px-6 py-3 bg-blue-100 rounded-l-2xl">Component</th>
                <th class="px-6 py-3 bg-blue-100">Minimal</th>
                <th class="px-6 py-3 bg-blue-100">Basic</th>
                <th class="px-6 py-3 bg-blue-100 rounded-r-2xl">Enthusiast</th>
            </tr>
        </thead>
        <tbody class="text-gray-800 text-base">
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">RAM</div>
                </td>
                <td class="px-6 py-4">4-8 GB</td>
                <td class="px-6 py-4">16 GB</td>
                <td class="px-6 py-4 rounded-r-2xl">32 GB+</td>
            </tr>
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">GPU VRAM</div>
                </td>
                <td class="px-6 py-4">Optional / CPU only</td>
                <td class="px-6 py-4">8 GB</td>
                <td class="px-6 py-4 rounded-r-2xl">16 GB+</td>
            </tr>
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">Storage</div>
                </td>
                <td class="px-6 py-4">10 GB free</td>
                <td class="px-6 py-4">50 GB SSD</td>
                <td class="px-6 py-4 rounded-r-2xl">1 TB+ SSD</td>
            </tr>
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">CPU</div>
                </td>
                <td class="px-6 py-4">2+ Cores</td>
                <td class="px-6 py-4">4+ Cores</td>
                <td class="px-6 py-4 rounded-r-2xl">8+ Cores</td>
            </tr>
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">Suitable Models</div>
                </td>
                <td class="px-6 py-4">tinyllama:1.1b<br>gemma:2b<br>phi3:mini</td>
                <td class="px-6 py-4">llama3.2:3b<br>mistral:7b<br>codellama:7b</td>
                <td class="px-6 py-4">llama3.1:70b<br>mixtral:8x7b<br>command-r:35b</td>
            </tr>
        </tbody>
    </table>
    <p>     </p>
            <p>Ollama serves as the primary interface for managing and running AI models locally. It simplifies the process of installing, configuring, and executing various language models. This guide demonstrates how to configure a development environment using Visual Studio Code (VS Code) and Jupyter Notebooks for interactive AI experimentation.</p>
            <p>The following sections cover:</p>
            <ul>
                <li>Software installation and configuration</li>
                <li>Development environment setup with proper isolation using virtual environments</li>
                <li>Initial model deployment and execution</li>
            </ul>
            <p>Each step includes detailed instructions and code examples. Prior programming experience is helpful but not required, as all necessary commands and configurations are provided.</p>
            <h5>A Quick Look at the Hardware</h5>
            <p>The hardware requirements table provides recommended specifications for different use cases. Performance of AI models is primarily determined by available Random Access Memory (RAM) and, optionally, Graphics Processing Unit (GPU) capabilities.</p>
            <ul>
                <li><strong>RAM:</strong> The system's primary memory allocation directly affects which model sizes can be loaded and executed. Larger models require proportionally more RAM.</li>
                <li><strong>GPU:</strong> NVIDIA graphics cards with CUDA support can significantly accelerate inference times. GPU acceleration is optional and provides performance benefits but is not required for basic operation.</li>
                <li><strong>CPU-Only Operation:</strong> GPU hardware is not mandatory. Ollama functions on CPU-only systems with standard configurations. Smaller models (e.g., <code>tinyllama:1.1b</code>, <code>phi3:mini</code>, <code>gemma:2b</code>) operate efficiently on systems with 4-8GB RAM. Response generation is slower compared to GPU-accelerated setups but remains practical for learning and development purposes.</li>
            </ul>
            <p>These specifications serve as guidelines rather than strict requirements. Entry-level hardware configurations are sufficient for experimentation with compact models and learning fundamental concepts. CUDA installation is only necessary for leveraging NVIDIA GPU acceleration and can be omitted for CPU-based workflows.</p>
            <h5>üöÄ Quick Start with A1-Terminal</h5>
            <p><strong>Want to skip manual setup?</strong> The A1-Terminal project includes automatic installation scripts that handle everything for you!</p>
            <p>The installation scripts automatically install:</p>
            <ul>
                <li>‚úÖ <strong>Python 3.11+</strong> (if not already present)</li>
                <li>‚úÖ <strong>Ollama</strong> service and API</li>
                <li>‚úÖ <strong>All required Python packages</strong> (customtkinter, ollama, PyYAML, requests, pyperclip)</li>
                <li>‚úÖ <strong>Test model</strong> (tinyllama:1.1b) to get started immediately</li>
                <li>‚ö†Ô∏è <strong>CUDA</strong> must be installed manually (only needed for NVIDIA GPU acceleration)</li>
            </ul>
            <p><strong>Perfect for:</strong> Beginners who want a working setup immediately, or anyone who prefers using a modern GUI instead of command-line tools.</p>
            <p>üìñ <strong>See the "A1-Terminal" tab above for complete installation instructions and features.</strong></p>
        </div>

        
<div class="ollama-navbar-content ollama-content-2">

#### Installation & Setup for developers

##### üì¶ Required Software Components

<div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 24px; margin: 24px 0;">
  
  <div style="background: linear-gradient(135deg, #1e3a8a 0%, #3b82f6 100%); padding: 24px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    <h3 style="color: white; margin-top: 0; display: flex; align-items: center; gap: 8px;">
      üêç <span>Python 3.8+</span>
    </h3>
    <ul style="color: white; line-height: 1.8;">
      <li>Download from <a href="https://www.python.org/downloads/" target="_blank" style="color: #93c5fd; text-decoration: underline;">python.org</a></li>
      <li>During installation: ‚úÖ Check "Add Python to PATH"</li>
      <li>Verify installation: <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 4px;">python --version</code></li>
      <li>Includes <strong>pip</strong> (Python package manager)</li>
    </ul>
  </div>

  <div style="background: linear-gradient(135deg, #7c3aed 0%, #a78bfa 100%); padding: 24px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    <h3 style="color: white; margin-top: 0; display: flex; align-items: center; gap: 8px;">
      ü¶ô <span>Ollama</span>
    </h3>
    <ul style="color: white; line-height: 1.8;">
      <li>Download from <a href="https://ollama.com/download" target="_blank" style="color: #e9d5ff; text-decoration: underline;">ollama.com</a></li>
      <li>Runs as background service on <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 4px;">localhost:11434</code></li>
      <li>Verify: <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 4px;">ollama --version</code></li>
      <li>Start service: <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 4px;">ollama serve</code> (automatic on Windows/macOS)</li>
    </ul>
  </div>

  <div style="background: linear-gradient(135deg, #0891b2 0%, #06b6d4 100%); padding: 24px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    <h3 style="color: white; margin-top: 0; display: flex; align-items: center; gap: 8px;">
      üíª <span>Visual Studio Code</span>
    </h3>
    <ul style="color: white; line-height: 1.8;">
      <li>Download from <a href="https://code.visualstudio.com/" target="_blank" style="color: #cffafe; text-decoration: underline;">code.visualstudio.com</a></li>
      <li><strong>Required Extensions</strong> (Ctrl+Shift+X):</li>
      <li style="margin-left: 20px;">‚Üí <strong>Python</strong> (ms-python.python) - Python IntelliSense & debugging</li>
      <li style="margin-left: 20px;">‚Üí <strong>Jupyter</strong> (ms-toolsai.jupyter) - Interactive notebooks</li>
    </ul>
  </div>

  <div style="background: linear-gradient(135deg, #15803d 0%, #22c55e 100%); padding: 24px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    <h3 style="color: white; margin-top: 0; display: flex; align-items: center; gap: 8px;">
      ‚ö° <span>CUDA Toolkit</span> <span style="font-size: 0.7em; background: rgba(255,255,255,0.3); padding: 2px 8px; border-radius: 4px;">Optional</span>
    </h3>
    <ul style="color: white; line-height: 1.8;">
      <li>Download from <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" style="color: #d9f99d; text-decoration: underline;">NVIDIA CUDA Downloads</a></li>
      <li>Required <strong>only</strong> for GPU acceleration with NVIDIA graphics cards</li>
      <li>Significantly improves inference speed</li>
      <li>Skip if using CPU-only or AMD GPUs</li>
    </ul>
  </div>

</div>

---

##### üîß Development Environment Setup

###### Step 1: Install VS Code Extensions
1. Open VS Code
2. Press `Ctrl+Shift+X` (Extensions)
3. Search and install: **Python** and **Jupyter**

###### Step 2: Create Virtual Environment
1. Open your project folder in VS Code
2. Press `Ctrl+Shift+P` (Command Palette)
3. Type: `Python: Create Environment`
4. Select **Venv**
5. Choose your Python interpreter

**Why virtual environments?** Isolates project dependencies, prevents version conflicts, and keeps your system Python clean.

###### Step 3: Install Jupyter Kernel
Open integrated terminal (`Ctrl+\``) and run:
```bash
pip install ipykernel
```
This enables Jupyter notebooks to use your virtual environment.

###### Step 4: Verify Ollama Service
Check if Ollama is running:
```bash
ollama list
```
If not running, start it:
```bash
ollama serve
```

###### Step 5: Download Your First Model
```bash
ollama pull tinyllama:1.1b
```
This downloads a small (~600 MB) test model. Other recommendations:
- `phi3:mini` (2 GB) - Good quality, balanced
- `llama3.2:3b` (2 GB) - Latest version
- `mistral:7b` (4 GB) - High quality

---

##### ‚úÖ Verification Checklist
- ‚úÖ Python installed and in PATH: `python --version`
- ‚úÖ Ollama service running: `ollama list`
- ‚úÖ At least one model downloaded: `ollama list`
- ‚úÖ VS Code extensions installed: Python + Jupyter
- ‚úÖ Virtual environment created and activated
- ‚úÖ Jupyter kernel installed: `pip list | grep ipykernel`

**üéØ Next Steps:** Your environment is ready! Start experimenting with Jupyter notebooks or launch A1-Terminal for a full-featured chat interface.

</div>

<div class="ollama-navbar-content ollama-content-3">

#### A1-Terminal: H-Term for AI Models

<div style="display: flex; align-items: center; gap: 12px; margin: 16px 0; padding: 16px; background: linear-gradient(135deg, #025709ff 0%, #05fa1aff 100%); border-radius: 12px; border: 1px solid #010202ff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
    <div style="background: #24292e; border-radius: 50%; width: 48px; height: 48px; display: flex; align-items: center; justify-content: center; box-shadow: 0 2px 6px rgba(0,0,0,0.2);">
        <svg width="24" height="24" fill="#0df005ff" viewBox="0 0 16 16">
            <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
        </svg>
    </div>
    <div>
        <div style="font-weight: bold; color: #24292e; font-size: 1.1em;">Version 1.0</div>
        <div style="margin-top: 4px;">
            <a href="https://github.com/Nr44suessauer/A1-Terminal/tree/Release-Version-1.0" target="_blank" style="color: #0366d6; text-decoration: none; font-weight: 500;">Nr44suessauer/A1-Terminal</a>
            <span style="color: #000000ff; margin-left: 8px;">| Branch: Release-Version-1.0</span>
        </div>
    </div>
</div>

##### üöÄ Automatic Installation (Recommended)

The installation script handles **everything automatically** - perfect for beginners or blank systems!

<div class="install-tabs">
  <input type="radio" name="auto-install" id="auto-windows" class="tab-input" checked>
  <input type="radio" name="auto-install" id="auto-linux" class="tab-input">
  
  <div class="tab-buttons">
    <label for="auto-windows" class="tab-button">ü™ü Windows</label>
    <label for="auto-linux" class="tab-button">üêßüçé Linux/macOS</label>
  </div>
  
  <div class="tab-content" id="auto-windows-content">

**Windows Installation**

```powershell
# 1. Clone repository
git clone https://github.com/Nr44suessauer/A1-Terminal.git
cd A1-Terminal

# 2. Run as Administrator (Right-click ‚Üí "Run as Administrator")
.\scripts\install.bat
```

**What the install.bat script does:**

- **Python Installation** - Checks for Python 3.11+, downloads and installs if missing
- **pip Update** - Updates Python package manager to latest version
- **Python Packages** - Installs all required packages from `requirements.txt`
- **Ollama Installation** - Downloads (~500 MB) and installs Ollama service
- **Test Model** - Downloads `tinyllama:1.1b` (~600 MB) for immediate use

**After installation:**

```powershell
cd a1_terminal_modular
.\start.bat
```

  </div>
  
  <div class="tab-content" id="auto-linux-content">

**Linux/macOS Installation**

```bash
# 1. Clone repository
git clone https://github.com/Nr44suessauer/A1-Terminal.git
cd A1-Terminal

# 2. Make executable and run
chmod +x scripts/install.sh
./scripts/install.sh
```

**What the install.sh script does:**

- **Python Installation** - Uses system package manager (apt/dnf/yum/brew)
- **pip Update** - Updates pip to latest version
- **Python Packages** - Installs all dependencies
- **Ollama** - Downloads and configures Ollama + test model

**After installation:**

```bash
cd a1_terminal_modular
./start.sh
```

  </div>
</div>

**‚è±Ô∏è Installation takes 5-10 minutes. Everything is automatic!**

---

##### üîß Manual Installation (Advanced)

For full control over each installation step:

<div class="install-tabs">
  <input type="radio" name="manual-install" id="manual-windows" class="tab-input" checked>
  <input type="radio" name="manual-install" id="manual-linux" class="tab-input">
  
  <div class="tab-buttons">
    <label for="manual-windows" class="tab-button">ü™ü Windows</label>
    <label for="manual-linux" class="tab-button">üêßüçé Linux/macOS</label>
  </div>
  
  <div class="tab-content" id="manual-windows-content">

**Windows Manual Installation**

```powershell
# 1. Install Ollama manually
# Visit https://ollama.com/download

# 2. Clone repository
git clone https://github.com/Nr44suessauer/A1-Terminal.git
cd A1-Terminal/a1_terminal_modular

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Download a model (optional)
ollama pull tinyllama:1.1b

# 5. Start application
python main.py
# Or use start script:
# .\start.bat
```

  </div>
  
  <div class="tab-content" id="manual-linux-content">

**Linux/macOS Manual Installation**

```bash
# 1. Install Ollama manually
# Visit https://ollama.com/download
# Or use: curl -fsSL https://ollama.com/install.sh | sh

# 2. Clone repository
git clone https://github.com/Nr44suessauer/A1-Terminal.git
cd A1-Terminal/a1_terminal_modular

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Download a model (optional)
ollama pull tinyllama:1.1b

# 5. Start application
python3 main.py
# Or use start script:
# ./start.sh
```

  </div>
</div>
---

#####  Project Structure

``````
A1-Terminal/
 scripts/
    install.bat          # Windows auto-installer
    install.sh           # Linux/macOS auto-installer
 start.bat               # Quick start (from root)
 a1_terminal_modular/
    main.py            # Entry point
    start.bat          # Windows start
    requirements.txt   # Dependencies
    a1_terminal_config.yaml  # Config
    sessions/          # Saved chats
    src/
        core/
           a1_terminal.py    # Main app
           ollama_manager.py # API client
        ui/
            ultimate_ui.py     # Modern UI
            chat_bubble.py     # Messages
            session_card.py    # Session list
            model_selector.py  # Model picker
            color_wheel.py     # Color picker
``````

---

#####  Recommended Models

After installation, **tinyllama:1.1b** is ready. Download more in the "Models" tab:

| Model | Size | RAM | Description |
|--------|-------|-----|-------------|
| **tinyllama:1.1b** | 600 MB | 4 GB |  Already installed! Fast |
| phi3:mini | 2 GB | 8 GB | Balanced quality |
| llama3.2:3b | 2 GB | 8 GB | Latest, very capable |
| mistral:7b | 4 GB | 12 GB | High quality |
| codellama:7b | 4 GB | 12 GB | For programming |
| mixtral:8x7b | 30 GB | 80+ GB | Top-tier performance |
---

#####  Configuration

Auto-created ``a1_terminal_config.yaml``:

``````yaml
# Colors
user_bg_color: "#003300"
user_text_color: "#00FF00"
ai_bg_color: "#1E3A5F"
ai_text_color: "white"

# Fonts
user_font: "Courier New"
ai_font: "Consolas"

# UI
ui_window_width: 1400
ui_window_height: 900

# Options
show_system_messages: true
auto_scroll_chat: true
``````


---

##### ‚ú® Key Features

- **Modular Architecture** - Clean separation of UI and business logic
- **Session Management** - Save and restore chat conversations
- **Fully Customizable** - Adjust colors, fonts, layout via config
- **Model Management** - Download & categorize by RAM requirements
- **100% Offline** - All models run locally, no cloud
- **Stop Function** - Interrupt generation or downloads
- **BIAS System** - Set system prompts for AI behavior
- **Export** - Save as JSON


---

#####  Troubleshooting

**Ollama Not Running:**
``````bash
ollama list      # Check status
ollama serve     # Start manually
``````

**App Won''t Start:**
``````bash
pip install -r requirements.txt --upgrade
python --version  # Needs 3.8+
``````

**Model Download Failed:**
- Check internet connection
- Try: ``ollama pull <model_name>``
- Check disk space
- Verify Ollama is running

---

#####  Documentation & Support

-  [Technical Documentation](https://github.com/Nr44suessauer/A1-Terminal/blob/main/DOCUMENTATION_EN.md){target="_blank"}
-  [GitHub Issues](https://github.com/Nr44suessauer/A1-Terminal/issues){target="_blank"}
-  [Discussions](https://github.com/Nr44suessauer/A1-Terminal/discussions){target="_blank"}

**Ready to start? Run the installation script!**

</div>

<div class="ollama-navbar-content ollama-content-4">

#### A1-Manual: Complete User Guide

##### üéØ Key Features Illustrated

- **[üñ•Ô∏è Console Initial Prompt](#console-initial-prompt)** 
- **[üîÑ Model Switching in Session](#model-switching)** 
- **[üí¨ Session Management](#session-management)**
- **[üé≠ Professional BIAS System](#bias-system)** 
- **[‚öôÔ∏è Configuration Screen](#configuration-screen)** 
- **[üìÅ Session Folder Structure (JSON)](#session-folder)** 
- **[üì• Download and Install](#download-install)**
- **[üé® Color Wheel & Session Settings](#color-wheel)** 

##### üì∏ Visual Walkthrough

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(500px, 1fr)); gap: 24px; margin: 24px 0;">
<div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
    <h4 id="console-initial-prompt" style="color: #60a5fa; margin-top: 0;">üñ•Ô∏è Console Initial Prompt</h4>
    <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/console_initial_prompt.png?raw=true" target="_blank">
        <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/console_initial_prompt.png?raw=true" alt="Console Initial Prompt" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
    </a>
    <div style="color: #cbd5e1; margin: 0;">
        <p style="margin-bottom: 16px;">The console provides real-time process monitoring through log outputs, showing the ideal workflow.</p>
        <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-size: 1.1em;">üéØ Console Features:</h5>
        <ul style="margin: 8px 0; padding-left: 20px;">
            <li style="margin-bottom: 8px;"><strong>Process Overview:</strong> Real-time log outputs show system status and operations</li>
            <li style="margin-bottom: 8px;"><strong>Error Detection:</strong> Errors are displayed and deviate from the ideal process shown here</li>
            <li style="margin-bottom: 8px;"><strong>Model Status:</strong> Terminal displays whether the AI model is currently processing</li>
            <li><strong>System Monitoring:</strong> Complete visibility into application state and performance</li>
        </ul>
    </div>
</div>


<div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
    <h4 id="model-switching" style="color: #60a5fa; margin-top: 0;">üîÑ Model Switching in Session</h4>
    <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/modelchange_in_session.png?raw=true" target="_blank">
        <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/modelchange_in_session.png?raw=true" alt="Model Change in Session" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
    </a>
    <div style="color: #cbd5e1; margin: 0;">
        <p style="margin-bottom: 16px;">Switch between different AI models mid-conversation to compare responses and capabilities.</p>
        <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-size: 1.1em;">üéØ Key Features:</h5>
        <ul style="margin: 8px 0; padding-left: 20px;">
            <li style="margin-bottom: 8px;"><strong>Multi-Session Support:</strong> Switch between multiple sessions with preserved content</li>
            <li style="margin-bottom: 8px;"><strong>Model Flexibility:</strong> Change models between or within sessions</li>
            <li style="margin-bottom: 8px;"><strong>Context Preservation:</strong> Automatic context retention when reopening sessions</li>
            <li style="margin-bottom: 8px;"><strong>Smart Management:</strong> Ollama handles conversation context automatically</li>
            <li><strong>Auto-Save:</strong> Sessions automatically saved after AI responses</li>
        </ul>
    </div>
</div>
<div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
    <h4 id="session-management" style="color: #60a5fa; margin-top: 0;">üí¨ Session Management</h4>
    <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/Session_astronaut.png?raw=true" target="_blank">
      <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/Session_astronaut.png?raw=true" alt="Session Example - Astronaut" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
    </a>
    <div style="color: #cbd5e1; margin: 0;">
        <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-siSze: 1.1em;">üéØ Session Features:</h5>
        <ul style="margin: 8px 0; padding-left: 20px;">
            <li style="margin-bottom: 8px;"><strong>Visual Session Identification:</strong> The chat window frame is colored in the session color</li>
            <li style="margin-bottom: 8px;"><strong>Customizable Colors & Names:</strong> Color and name are set using the gear icon next to the session</li>
            <li style="margin-bottom: 8px;"><strong>BIAS Settings:</strong> The session BIAS setting is located in the bottom left; if no BIAS is set, this field is empty</li>
            <li style="margin-bottom: 8px;"><strong>JSON Storage:</strong> Sessions are stored in JSON format in the sessions folder, which opens when clicking the session-folder button</li>
            <li><strong>Session Preservation:</strong> All your conversations are preserved and can be restored anytime</li>
        </ul>
    </div>
</div>

<div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
    <h4 id="bias-system" style="color: #60a5fa; margin-top: 0;">üé≠ Professional BIAS System</h4>
    <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/session_Prof_BIAS.png?raw=true" target="_blank">
        <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/session_Prof_BIAS.png?raw=true" alt="Professional BIAS Configuration" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
    </a>
    <div style="color: #cbd5e1; margin: 0;">
            <p style="margin-bottom: 16px;">Configure system prompts (BIAS) to define AI behavior and personality for each session.</p>
            <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-size: 1.1em;">üéØ BIAS System Features:</h5>
            <ul style="margin: 8px 0; padding-left: 20px;">
                    <li style="margin-bottom: 8px;"><strong>Professional Conversations:</strong> BIAS enables specialized technical discussions with AI - response quality depends on the model and hardware used</li>
                    <li style="margin-bottom: 8px;"><strong>Basic Queries:</strong> Fundamental questions can be easily asked and answered</li>
                    <li style="margin-bottom: 8px;"><strong>Critical Verification:</strong> Always remain skeptical and verify the received answers - AI responses should be fact-checked</li>
                    <li><strong>Session-Specific:</strong> Each session can have its own BIAS configuration for targeted conversations</li>
            </ul>
    </div>
</div>
<div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
    <h4 id="configuration-screen" style="color: #60a5fa; margin-top: 0;">‚öôÔ∏è Configuration Screen</h4>
    <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/screen_Config.png?raw=true" target="_blank">
        <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/screen_Config.png?raw=true" alt="Configuration Screen" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
    </a>
    <div style="color: #cbd5e1; margin: 0;">
            <p style="margin-bottom: 16px;">Customize your A1-Terminal experience with colors, fonts, and UI preferences in the configuration screen.</p>
            <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-size: 1.1em;">üéØ Configuration Options:</h5>
            <ul style="margin: 8px 0; padding-left: 20px;">
                    <li style="margin-bottom: 8px;"><strong>Console Appearance:</strong> Adjust colors, shape, and size of the console interface</li>
                    <li style="margin-bottom: 8px;"><strong>Debug/System Outputs:</strong> Toggle debug and system message visibility on/off</li>
                    <li style="margin-bottom: 8px;"><strong>Auto-Scroll:</strong> Enable/disable automatic scrolling to the latest message</li>
                    <li><strong>Apply Changes:</strong> Clicking "Apply" triggers a restart script that closes and reopens the software automatically</li>
            </ul>
    </div>
</div>

<div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
    <h4 id="session-folder" style="color: #60a5fa; margin-top: 0;">üìÅ Session Folder Structure (JSON)</h4>
    <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/folder_Session_Json.png?raw=true" target="_blank">
        <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/folder_Session_Json.png?raw=true" alt="Session JSON Storage" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
    </a>
            <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-size: 1.1em;">üîí Privacy & Data Protection:</h5>
            <ul style="margin: 8px 0; padding-left: 20px;">
                    <li style="margin-bottom: 8px;"><strong>Local Session Logs:</strong> Complete conversation histories for retrieval, sharing, and analysis</li>
                    <li style="margin-bottom: 8px;"><strong>Valuable Personal Data:</strong> These datasets reveal thinking patterns and personality traits - highly valuable to platforms like OpenAI</li>
                    <li style="margin-bottom: 8px;"><strong>Full Local Control:</strong> Your conversations remain on your machine, never sent to external servers</li>
                    <li><strong>Data Sovereignty:</strong> You decide what happens with your data - share, analyze, or keep private</li>
            </ul>
    </div>
<div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
    <h4 id="download-install" style="color: #60a5fa; margin-top: 0;">üì• Download and Install</h4>
    <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/download_and_install.png?raw=true" target="_blank">
        <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/download_and_install.png?raw=true" alt="Download and Install" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
    </a>
    <div style="color: #cbd5e1; margin: 0;">
        <p style="margin-bottom: 16px;">The model download interface allows you to browse and install AI models directly from the application.</p>
        <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-size: 1.1em;">üéØ Download Features:</h5>
        <ul style="margin: 8px 0; padding-left: 20px;">
            <li style="margin-bottom: 8px;"><strong>Model Browser:</strong> Browse available models categorized by size and performance</li>
            <li style="margin-bottom: 8px;"><strong>Progress Tracking:</strong> Real-time download progress with speed indicators</li>
            <li style="margin-bottom: 8px;"><strong>Space Requirements:</strong> Clear disk space and RAM requirements for each model</li>
            <li style="margin-bottom: 8px;"><strong>Model Folder Access:</strong> Direct access to the local model storage folder for file management</li>
            <li><strong>One-Click Install:</strong> Simple installation process directly from the interface</li>
        </ul>
    </div>
</div>
    <div style="background: #1e293b; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.2);">
        <h4 id="color-wheel" style="color: #60a5fa; margin-top: 0;">üé® Color Wheel & Session Settings</h4>
        <a href="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/colorwheel.png?raw=true" target="_blank">
            <img src="https://github.com/Nr44suessauer/nr44suessauer.github.io/blob/main/nuxt-app/assets/pictures/A1-Terminal/colorwheel.png?raw=true" alt="Color Wheel Interface" style="width: 100%; border-radius: 8px; margin-bottom: 12px; cursor: pointer; transition: transform 0.3s;">
        </a>
        <div style="color: #cbd5e1; margin: 0;">
            <p style="margin-bottom: 16px;">Access this customization window by clicking the gear icon next to any session in the session list.</p>
            <h5 style="color: #60a5fa; margin: 16px 0 8px 0; font-size: 1.1em;">üéØ Session Customization Features:</h5>
            <ul style="margin: 8px 0; padding-left: 20px;">
                <li style="margin-bottom: 8px;"><strong>Session Name:</strong> Set or change the session name in the text field at the top</li>
                <li style="margin-bottom: 8px;"><strong>Visual Color Picker:</strong> Intuitive color wheel interface for precise color selection</li>
                <li style="margin-bottom: 8px;"><strong>Real-Time Preview:</strong> See color changes instantly as you adjust the wheel</li>
                <li style="margin-bottom: 8px;"><strong>Session Theming:</strong> Customize individual session colors for easy identification</li>
                <li><strong>Persistent Settings:</strong> Both color and name choices are saved and restored when reopening sessions</li>
            </ul>
        </div>
    </div>
</div>
</div>


<style>
/* Main Tabs (How AI Works, Preparation, etc.) */
.ollama-navbar-tabs {
  width: 100%;
}
.ollama-tab-radio {
  display: none;
}
.ollama-navbar-btn {
  background: #007bff;
  color: white;
  border: none;
  border-radius: 6px 6px 0 0;
  padding: 12px 32px;
  font-size: 1em;
  font-weight: bold;
  cursor: pointer;
  transition: background 0.2s;
  outline: none;
}
.ollama-navbar-btn.active,
.ollama-navbar-btn:hover {
  background: #0056b3;
}
.ollama-navbar-content {
  display: none;
}
#ollama-tab-0:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-0"],
#ollama-tab-1:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-1"],
#ollama-tab-2:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-2"],
#ollama-tab-3:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-3"],
#ollama-tab-4:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-4"] {
  background: #0056b3;
}
#ollama-tab-0:checked ~ .ollama-content-0,
#ollama-tab-1:checked ~ .ollama-content-1,
#ollama-tab-2:checked ~ .ollama-content-2,
#ollama-tab-3:checked ~ .ollama-content-3,
#ollama-tab-4:checked ~ .ollama-content-4 {
  display: block;
}

/* Installation Tabs (Windows/Linux) */
.install-tabs {
  margin: 24px 0;
}

.install-tabs .tab-input {
  display: none;
}

.install-tabs .tab-buttons {
  display: flex;
  gap: 8px;
  margin-bottom: 0;
  border-bottom: 2px solid #dee2e6;
}

.install-tabs .tab-button {
  background: #6c757d;
  color: white;
  border: none;
  border-radius: 8px 8px 0 0;
  padding: 12px 32px;
  font-size: 1em;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s ease;
  border-bottom: 3px solid transparent;
}

.install-tabs .tab-button:hover {
  background: #5a6268;
  transform: translateY(-2px);
}

.install-tabs .tab-content {
  display: none;
  background: transparent;
  border-radius: 0 8px 8px 8px;
  padding: 24px;
  border: 2px solid currentColor;
  border-top: none;
  animation: fadeIn 0.3s ease;
  color: inherit;
  opacity: 0.85;
}

.install-tabs .tab-content strong {
  color: inherit;
  font-weight: 700;
}

.install-tabs .tab-content code {
  background: rgba(0, 0, 0, 0.3);
  padding: 2px 6px;
  border-radius: 4px;
  color: inherit;
}

@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(-10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Automatic Installation Tabs */
#auto-windows:checked ~ .tab-buttons label[for="auto-windows"],
#manual-windows:checked ~ .tab-buttons label[for="manual-windows"] {
  background: #0056b3;
  border-bottom-color: #0056b3;
}

#auto-linux:checked ~ .tab-buttons label[for="auto-linux"],
#manual-linux:checked ~ .tab-buttons label[for="manual-linux"] {
  background: #0056b3;
  border-bottom-color: #0056b3;
}

#auto-windows:checked ~ #auto-windows-content,
#auto-linux:checked ~ #auto-linux-content,
#manual-windows:checked ~ #manual-windows-content,
#manual-linux:checked ~ #manual-linux-content {
  display: block;
}
</style>
