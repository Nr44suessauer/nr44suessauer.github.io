---
cover: https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA
date: 2024-07-28T00:00:00.000Z
description: How To run AI Models on your own PC | Beginner Guide
layout: article_backup
order: 2
---

<style>
.ollama-navbar {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
}
.ollama-navbar-btn {
    background: #007bff;
    color: white;
    border: none;
    border-radius: 6px 6px 0 0;
    padding: 12px 32px;
    font-size: 1em;
    font-weight: bold;
    cursor: pointer;
    transition: background 0.2s;
    outline: none;
}
.ollama-navbar-btn.active,
.ollama-navbar-btn:hover {
    background: #0056b3;
}
.ollama-navbar-content {
    background: #000000ff;
    border-radius: 0 0 8px 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.08);
    padding: 24px;
    margin-top: -2px;
    border-top: 1px solid #ddd;
}
</style>


<div class="ollama-navbar-tabs">
<input type="radio" name="ollama-tab" id="ollama-tab-0" class="ollama-tab-radio" checked>
<input type="radio" name="ollama-tab" id="ollama-tab-1" class="ollama-tab-radio">
<input type="radio" name="ollama-tab" id="ollama-tab-2" class="ollama-tab-radio">
<div class="ollama-navbar">
    <label class="ollama-navbar-btn" for="ollama-tab-0">ğŸ“‹ Preparation</label>
    <label class="ollama-navbar-btn" for="ollama-tab-1">âš™ï¸ Setup</label>
    <label class="ollama-navbar-btn" for="ollama-tab-2">ğŸ’¬ A1-Terminal</label>
</div>
<div class="ollama-navbar-content ollama-content-0">
        <div style="margin-top: 24px; margin-bottom: 24px;">
            <h4>Before You Begin</h4>
            <p>This guide provides an introduction to running AI models locally on your computer. You'll learn how to work with Large Language Models (LLMs) using tools like Ollama, Jupyter Notebooks, and Python virtual environments.</p>
            <p>Running AI models locally offers several advantages: full control over your data, enhanced privacy, no dependency on internet connectivity, and no recurring cloud service costs. This approach is particularly valuable for academic work, research projects, and learning the fundamentals of AI implementation.</p>
        </div>
    <table>
        <tr>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/V2oVVCsPaNxkGlhMMz5AxhgzgEvkZRmyQf3x22R5ebQ/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuc3RpY2twbmcu/Y29tL2ltYWdlcy82/MmE3OTA2Y2U0MmQ3/MjlkOTI4YjE3NTcu/cG5n" alt="VS Code Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://code.visualstudio.com/download" target="_blank" rel="noopener">VS Code</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://www.python.org/static/community_logos/python-logo.png" alt="Python Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://www.python.org/downloads/" target="_blank" rel="noopener">Python</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/8xshJuTEcdDh4ATRrUMvxIB39O6v3yf7382wqSkPcWg/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9jZG4u/YnJhbmRmZXRjaC5p/by9pZHJSRG1aMl9G/L3cvMTgwL2gvMTgw/L3RoZW1lL2xpZ2h0/L2xvZ28ucG5nP2M9/MWJ4aWQ2NE11cDdh/Y3pld1NBWU1YJnQ9/MTc0Nzc0NDA3MTE3/OA" alt="Ollama Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://ollama.com/download" target="_blank" rel="noopener">Ollama</a>
            </td>
            <td align="center" style="width: 180px;">
                <img src="https://imgs.search.brave.com/zLvtdX6w_dNUl6wAzFN-0BCdZQrJu7VkSySkbESjtsc/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy9i/L2I5L052aWRpYV9D/VURBX0xvZ28uanBn" alt="CUDA Logo" style="max-width: 100px; max-height: 80px; display: block; margin: 0 auto;">
                <br>
                <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA</a>
            </td>
        </tr>
    </table>
    <table class="w-full text-left table-auto border-separate [border-spacing:0_0.75rem]">
        <thead class="text-gray-600 uppercase text-sm font-semibold">
            <tr>
                <th class="px-6 py-3 bg-blue-100 rounded-l-2xl">Component</th>
                <th class="px-6 py-3 bg-blue-100">Basic</th>
                <th class="px-6 py-3 bg-blue-100 rounded-r-2xl">Enthusiast</th>
            </tr>
        </thead>
        <tbody class="text-gray-800 text-base">
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">RAM</div>
                </td>
                <td class="px-6 py-4">16 GB</td>
                <td class="px-6 py-4 rounded-r-2xl">32 GB+</td>
            </tr>
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">GPU VRAM</div>
                </td>
                <td class="px-6 py-4">8 GB</td>
                <td class="px-6 py-4 rounded-r-2xl">16 GB+</td>
            </tr>
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">Storage</div>
                </td>
                <td class="px-6 py-4">50 GB SSD</td>
                <td class="px-6 py-4 rounded-r-2xl">1 TB+ SSD</td>
            </tr>
            <tr class="bg-gray-50 hover:bg-gray-100 transition-colors duration-200 rounded-2xl">
                <td class="px-6 py-4 rounded-l-2xl">
                    <div class="font-medium">CPU</div>
                </td>
                <td class="px-6 py-4">4+ Cores</td>
                <td class="px-6 py-4 rounded-r-2xl">8+ Cores</td>
            </tr>
        </tbody>
    </table>
    <p>     </p>
            <p>Ollama serves as the primary interface for managing and running AI models locally. It simplifies the process of installing, configuring, and executing various language models. This guide demonstrates how to configure a development environment using Visual Studio Code (VS Code) and Jupyter Notebooks for interactive AI experimentation.</p>
            <p>The following sections cover:</p>
            <ul>
                <li>Software installation and configuration</li>
                <li>Development environment setup with proper isolation using virtual environments</li>
                <li>Initial model deployment and execution</li>
            </ul>
            <p>Each step includes detailed instructions and code examples. Prior programming experience is helpful but not required, as all necessary commands and configurations are provided.</p>
            <h5>A Quick Look at the Hardware</h5>
            <p>The hardware requirements table provides recommended specifications for different use cases. Performance of AI models is primarily determined by available Random Access Memory (RAM) and, optionally, Graphics Processing Unit (GPU) capabilities.</p>
            <ul>
                <li><strong>RAM:</strong> The system's primary memory allocation directly affects which model sizes can be loaded and executed. Larger models require proportionally more RAM.</li>
                <li><strong>GPU:</strong> NVIDIA graphics cards with CUDA support can significantly accelerate inference times. GPU acceleration is optional and provides performance benefits but is not required for basic operation.</li>
                <li><strong>CPU-Only Operation:</strong> GPU hardware is not mandatory. Ollama functions on CPU-only systems with standard configurations. Smaller models (e.g., <code>tinyllama:1.1b</code>, <code>phi3:mini</code>, <code>gemma:2b</code>) operate efficiently on systems with 4-8GB RAM. Response generation is slower compared to GPU-accelerated setups but remains practical for learning and development purposes.</li>
            </ul>
            <p>These specifications serve as guidelines rather than strict requirements. Entry-level hardware configurations are sufficient for experimentation with compact models and learning fundamental concepts. CUDA installation is only necessary for leveraging NVIDIA GPU acceleration and can be omitted for CPU-based workflows.</p>
        </div>

        
<div class="ollama-navbar-content ollama-content-1">
    <h4>Installation Process</h4>
    <ol>
        <li>Install VS Code extensions: <strong>Python</strong> and <strong>Jupyter</strong> (Ctrl+Shift+X in VS Code).</li>
        <li>Create a virtual Python environment: Open folder in VS Code, use <code>Python: Create Environment</code> (Ctrl+Shift+P), select <code>Venv</code>.</li>
        <li>Install Jupyter kernel: Open terminal, run <code>pip install ipykernel</code> in your virtual environment.</li>
    </ol>
    <p>Start Ollama after installation and check if the service is running. Make sure Python and CUDA are set up correctly for optimal AI model performance. Configure your environment variables and test the installation.</p>
</div>

<div class="ollama-navbar-content ollama-content-2">

#### A1-Terminal: Professioneller Chat-Client fÃ¼r lokale AI-Modelle

**ğŸš€ Ki-whisperer - Modulare Chat-Anwendung mit Ollama**  
[GitHub Repository â†’](https://github.com/Nr44suessauer/Ki-whisperer)

##### âœ¨ Hauptfeatures

- **ğŸ¯ Modulare Architektur** â€“ Saubere Code-Struktur mit klarer Trennung der Komponenten
- **ğŸš€ Echtzeit-Streaming** â€“ Sehen Sie AI-Antworten live Token-fÃ¼r-Token
- **ğŸ’¾ Session-Management** â€“ Speichern und laden Sie Chat-Sitzungen automatisch
- **ğŸ¨ VollstÃ¤ndig anpassbar** â€“ Individualisieren Sie Farben, Schriftarten und Layout
- **ğŸ“Š Model-Management** â€“ Download, Auswahl und Kategorisierung nach RAM-Anforderungen
- **ğŸ”„ Offline-fÃ¤hig** â€“ Alle Modelle laufen komplett lokal auf Ihrem PC

##### ğŸ—ï¸ Projekt-Struktur

```
Ki-whisperer/
â”œâ”€â”€ a1_terminal_modular/       # Modulare Version (empfohlen)
â”‚   â”œâ”€â”€ main.py               # Einstiegspunkt
â”‚   â”œâ”€â”€ start.bat             # Windows-Launcher
â”‚   â”œâ”€â”€ requirements.txt      # Python-Dependencies
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ core/             # Kernlogik
â”‚       â”‚   â”œâ”€â”€ a1_terminal.py      # Hauptanwendung (3200+ Zeilen)
â”‚       â”‚   â””â”€â”€ ollama_manager.py   # API-Client (320 Zeilen)
â”‚       â””â”€â”€ ui/               # UI-Komponenten
â”‚           â”œâ”€â”€ color_wheel.py      # RGB-FarbwÃ¤hler
â”‚           â”œâ”€â”€ chat_bubble.py      # Chat-Nachrichten
â”‚           â””â”€â”€ categorized_combobox.py  # Dropdown-MenÃ¼
â””â”€â”€ ki_whisperer_config.yaml  # Zentrale Konfiguration
```

##### ğŸš€ Schnellstart

1. **Ollama installieren** â€“ Besuchen Sie [ollama.ai](https://ollama.ai/) und installieren Sie Ollama
2. **Repository klonen**
   ```bash
   git clone https://github.com/Nr44suessauer/Ki-whisperer.git
   cd Ki-whisperer/a1_terminal_modular
   ```
3. **Dependencies installieren**
   ```bash
   pip install -r requirements.txt
   ```
4. **Anwendung starten**
   ```bash
   .\start.bat
   # oder
   python main.py
   ```

##### ğŸ¨ Intelligente Model-Kategorisierung

Modelle sind farblich nach RAM-Anforderungen gruppiert:

- **ğŸŸ¢ Klein (< 4GB RAM)** â€“ 18 Modelle: `tinyllama:1.1b`, `phi3:mini`, `gemma:2b`
- **ğŸŸ¡ Mittel (4-8GB RAM)** â€“ 32 Modelle: `llama3.2:3b`, `mistral:7b`, `codellama:7b`
- **ğŸŸ  GroÃŸ (8-16GB RAM)** â€“ 3 Modelle: `llama2:13b`, `solar:10.7b`, `starcode:15b`
- **ğŸ”´ Sehr GroÃŸ (16GB+ RAM)** â€“ 7 Modelle: `llama2:70b`, `mixtral:8x7b`, `codellama:34b`

##### ğŸŒ Live-API Integration

- **Echte Live-Daten** â€“ Keine statische Liste, immer die neuesten Modelle
- **Automatische Updates** â€“ Neue Modelle erscheinen sofort nach Release
- **60+ Aktuelle Modelle** â€“ VollstÃ¤ndige Ollama-Bibliothek verfÃ¼gbar
- **Fallback-System** â€“ Robuste Offline-Liste bei API-Problemen

##### ğŸ›ï¸ Erweiterte Features

- **BIAS-System** â€“ Session-spezifischer Kontext (Background Information And System instructions)
- **Anti-Redundanz-System** â€“ Saubere Ausgaben ohne nervige Wiederholungen
- **RGB-FarbwÃ¤hler** â€“ Visueller FarbwÃ¤hler mit Live-Preview
- **Export-Funktionen** â€“ Markdown und JSON-Export von Chat-Sessions
- **Performance-Monitoring** â€“ Response-Time, Token-Rate und Memory-Usage
- **Threading** â€“ Alle API-Calls laufen im Hintergrund, UI bleibt reaktiv

##### ğŸ“– Dokumentation

Die vollstÃ¤ndige technische Dokumentation mit API-Referenz, Konfigurationsoptionen und Best Practices finden Sie hier:  
[**â¡ï¸ VollstÃ¤ndige Dokumentation**](https://github.com/Nr44suessauer/Ki-whisperer/blob/main/DOKUMENTATION.md)

**ğŸ’¡ Tipp:** A1-Terminal ist perfekt fÃ¼r alle, die ihre Ollama-Modelle mit einer modernen GUI nutzen mÃ¶chten. Es bietet deutlich mehr Komfort als das Terminal und ist ideal fÃ¼r produktive Arbeit mit lokalen AI-Modellen.

</div>
</div>
<style>
.ollama-navbar-tabs {
  width: 100%;
}
.ollama-tab-radio {
  display: none;
}
.ollama-navbar-btn {
  /* ...bestehendes Styling... */
}
.ollama-navbar-content {
  display: none;
}
#ollama-tab-0:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-0"],
#ollama-tab-1:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-1"],
#ollama-tab-2:checked ~ .ollama-navbar .ollama-navbar-btn[for="ollama-tab-2"] {
  background: #0056b3;
}
#ollama-tab-0:checked ~ .ollama-content-0,
#ollama-tab-1:checked ~ .ollama-content-1,
#ollama-tab-2:checked ~ .ollama-content-2 {
  display: block;
}
</style>
